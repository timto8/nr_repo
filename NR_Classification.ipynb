{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "87f97d08",
   "metadata": {},
   "source": [
    "# Initial Network File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb21d4de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sys\n",
    "import random as rd\n",
    "np.set_printoptions(threshold=sys.maxsize)\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import nn, optim\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf8fb406",
   "metadata": {},
   "source": [
    "## Initial Data Load+Visualise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05a2d687",
   "metadata": {},
   "source": [
    "Data is stored in the same directory as this .ipynb file and loaded into the notebook below. A loop is run over each energy interval and a variable is allocated, using the `globals()` function, to the data from each file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36bb0b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in np.arange(50,201,5):\n",
    "    globals()['C_'+str(i)+'keV']=np.load('C_'+str(i)+'keV.npy') #"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c2f116b",
   "metadata": {},
   "source": [
    "The shape of each loaded data file is output below - an array containing $5000$ $97\\times97$ matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1f4b9d7",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "np.shape(C_200keV)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35ebef0c",
   "metadata": {},
   "source": [
    "Each matrix can be selected using its index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f2f6071",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(C_100keV[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6978e10",
   "metadata": {},
   "source": [
    "We can print out one of the matrices for the `C_85keV` energy to visualise how the data is distributed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cf3948a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# C_85keV[500]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ff0137a",
   "metadata": {},
   "source": [
    "The vast majority of pixels have entries of 0. Different energies and indices show a similar distribution. This means the recoil track is small relative to the image size."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3bd28de",
   "metadata": {},
   "source": [
    "Difficulty is encountered loading the data for Flourine. A loop is run below to identify at which energy this error occurs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d1c2881",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in np.arange(50,201,5):\n",
    "    try:\n",
    "        np.load('F_'+str(i)+'keV.npy')\n",
    "    except:\n",
    "        print('There is a problem with F_'+str(i)+'keV.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "614d413c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.load('F_200keV.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00d4a532",
   "metadata": {},
   "source": [
    "There is an error with the `200keV` file for Flourine. I also noticed that all the other files are `376.4MB` but `F_200keV.npy` is `146.4MB`. This is due to a problem with the initial file (according to Tim). I will only use up to `195keV` for both Carbon and Flourine. I wouldn't want to train a model on `200keV` for one element but not the other (it could just assume that an energy of `200keV` makes something Carbon rather than finding the more complex features)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aac6522",
   "metadata": {},
   "source": [
    "I plot out a random matrix from one of the energies to visualise it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b0684d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(15,10))\n",
    "plt.matshow(C_150keV[10],fignum=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90413f35",
   "metadata": {},
   "source": [
    "## Preparing Data for Pytorch and CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c7de20c",
   "metadata": {},
   "source": [
    "For a learning algorithm, it will be necesarry to have an input (X data) containing a number of $97\\times97$ matrices for each energy. The output will be a label, either Carbon or Flourine. The model will compare its output with the actual labels and adjust accordingly to minimise loss. \n",
    "\n",
    "In order to quantify each label, the common 'one-hot encoding' method will be used. C elements will have the label `[1, 0]` and F will be `[0, 1]`. Tuples of this form will be the ground truth labels and the model output. I will have to ensure that the model outputs an array of shape `[2]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb3d3f77",
   "metadata": {},
   "outputs": [],
   "source": [
    "im_dat, el_labs, en_labs = [],[],[]\n",
    "for i in np.arange(50,196,5): # run over all energies\n",
    "    C_dat = np.load('C_'+str(i)+'keV.npy')\n",
    "    for n in range(np.shape(C_dat)[0]):\n",
    "        im_dat.append(C_dat[n]),el_labs.append([1,0]),en_labs.append(i)\n",
    "    F_dat = np.load('F_'+str(i)+'keV.npy')\n",
    "    for n in range(np.shape(F_dat)[0]):\n",
    "        im_dat.append(F_dat[n]),el_labs.append([0,1]),en_labs.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80c097a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(im_dat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8138ca88",
   "metadata": {},
   "source": [
    "$300000$ is the number we want. Aka $5000\\times2\\times30$ (there are $30$ different energies for both C and F)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ff376c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(el_labs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "677db451",
   "metadata": {},
   "outputs": [],
   "source": [
    "el_labs[154999]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef21e806",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(en_labs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72be66a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#writing class for custom dataset\n",
    "class CustomDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, X_data, y_data):\n",
    "        self.X_data = X_data #initialises X_data\n",
    "        self.y_data = y_data #initialises y_data\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        return self.X_data[index], self.y_data[index] #returns one training example\n",
    "        \n",
    "    def __len__ (self):\n",
    "        return len(self.X_data) #returns length of the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e103c2e",
   "metadata": {},
   "source": [
    "**I need to split into training and testing data first**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb896ad3",
   "metadata": {},
   "source": [
    "# Classifying Energy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf0e3241",
   "metadata": {},
   "source": [
    "X data is the 97x97 arrays, 'im_dat'\n",
    "\n",
    "y data in this case is list of energies, 'en_labs'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b2c95df",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(im_dat, el_labs, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "895a3b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "419ff385",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0def706",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49c07220",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75fac92d",
   "metadata": {},
   "source": [
    "In order to apply a 2-dimensional CNN, it will be necessary to employ PyTorch's `nn.Conv2d()` function, which applies a 2D convolution over the input. `nn.Conv2d()` expects the input to be of the shape `[batch_size, input_channels, matrix_height, matrix_width]`, so we must ensure that the data is in this shape. Initially the data is in the shape `[batch_size, matrix_height, matrix_width]`, so an additional dimension must be added containing the number of input channels, which in this case is 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "848187fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain_tensor = torch.from_numpy(np.array(X_train)) #converts numpy array to torch tensor\n",
    "xtrain_sqz = xtrain_tensor.unsqueeze(1) #adds new dimension along axis 1\n",
    "print(xtrain_sqz.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e850a3c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "ytrain_tensor = torch.from_numpy(np.array(y_train))\n",
    "ytrain_sqz = ytrain_tensor.unsqueeze(1)\n",
    "print(ytrain_sqz.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9448c3c7",
   "metadata": {},
   "source": [
    "Now for test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5d3b34c",
   "metadata": {},
   "outputs": [],
   "source": [
    "xtest_tensor = torch.from_numpy(np.array(X_test)) #converts numpy array to torch tensor\n",
    "xtest_sqz = xtest_tensor.unsqueeze(1) #adds new dimension along axis 1\n",
    "print(xtest_sqz.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4163b6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ytest_tensor = torch.from_numpy(np.array(y_test))\n",
    "ytest_sqz = ytest_tensor.unsqueeze(1)\n",
    "print(ytest_sqz.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9f6faac",
   "metadata": {},
   "source": [
    "Applying dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9606970",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Converting data to floats and applying dataset class\n",
    "train_data = CustomDataset(xtrain_sqz, ytrain_sqz)\n",
    "test_data = CustomDataset(xtest_sqz, ytest_sqz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70f9cda0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Converting data to floats and applying dataset class\n",
    "# train_data = CustomDataset(xtrain_sqz.float(), ytrain_sqz.float())\n",
    "# test_data = CustomDataset(xtest_sqz.float(), ytest_sqz.float())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5d7cd70",
   "metadata": {},
   "source": [
    "The Kernel dies when running the above. I have too much data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87a5cd82",
   "metadata": {},
   "source": [
    "## Reducing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81ed1a80",
   "metadata": {},
   "outputs": [],
   "source": [
    "atest=rd.choices(X_train, k=3)\n",
    "np.shape(atest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34e6b973",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_small, X_test_small, y_train_small, y_test_small = rd.choices(X_train, k=2400), rd.choices(X_test, k=600),\\\n",
    "rd.choices(y_train, k=2400), rd.choices(y_test, k=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "368caf6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain_tensor = torch.from_numpy(np.array(X_train_small)) #converts numpy array to torch tensor\n",
    "xtrain_sqz = xtrain_tensor.unsqueeze(1) #adds new dimension along axis 1\n",
    "print(xtrain_sqz.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79089466",
   "metadata": {},
   "outputs": [],
   "source": [
    "ytrain_tensor = torch.from_numpy(np.array(y_train_small))\n",
    "ytrain_sqz = ytrain_tensor.unsqueeze(1)\n",
    "print(ytrain_sqz.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb9bfec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "xtest_tensor = torch.from_numpy(np.array(X_test_small)) #converts numpy array to torch tensor\n",
    "xtest_sqz = xtest_tensor.unsqueeze(1) #adds new dimension along axis 1\n",
    "print(xtest_sqz.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "658e8c77",
   "metadata": {},
   "outputs": [],
   "source": [
    "ytest_tensor = torch.from_numpy(np.array(y_test_small))\n",
    "ytest_sqz = ytest_tensor.unsqueeze(1)\n",
    "print(ytest_sqz.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9f9af30",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Converting data to floats and applying dataset class\n",
    "train_data = CustomDataset(xtrain_sqz.float(), ytrain_tensor.float()) #changed from sqz to float to long\n",
    "test_data = CustomDataset(xtest_sqz.float(), ytest_tensor.float())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "776f502a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining dataloader class\n",
    "train_loader = DataLoader(dataset=train_data, batch_size=50, shuffle=True)\n",
    "train_loader_iter = iter(train_loader)\n",
    "# print(next(train_loader_iter))\n",
    "\n",
    "#testing to see shape of output\n",
    "test=next(train_loader_iter)\n",
    "test[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9981588",
   "metadata": {},
   "outputs": [],
   "source": [
    "#same as above but for test data\n",
    "test_loader = DataLoader(dataset=test_data, batch_size=50, shuffle=True)\n",
    "test_loader_iter = iter(test_loader)\n",
    "\n",
    "test=next(test_loader_iter)\n",
    "test[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40e07955",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "231d6a98",
   "metadata": {},
   "source": [
    "Code for finding maxpooling output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f56c5e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = torch.randn(50, 1, 97, 97)\n",
    "m1 = nn.Conv2d(1, 5, kernel_size=(5,5))\n",
    "o1 = m1(inp)\n",
    "print(np.shape(o1))\n",
    "m2 = nn.MaxPool2d(kernel_size=(3,3))\n",
    "o2 = m2(o1)\n",
    "print(np.shape(o2))\n",
    "m3 = nn.Conv2d(5, 10, kernel_size=(5,5))\n",
    "o3 = m3(o2)\n",
    "print(np.shape(o3))\n",
    "m4 = nn.MaxPool2d(kernel_size=(3,3))\n",
    "o4 = m4(o3)\n",
    "print(np.shape(o4))\n",
    "o5 = o4.view(-1, 810)\n",
    "print(np.shape(o5))\n",
    "m6 = nn.Linear(810,80)\n",
    "o6 = m6(o5)\n",
    "print(np.shape(o6))\n",
    "m7 = nn.Linear(80,2)\n",
    "o7 = m7(o6)\n",
    "print(np.shape(o7))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fd6be22",
   "metadata": {},
   "source": [
    "Previously I was going into the batch dimension by having the wrong dimensions for the flattening layer. I didn't realise that for the flattening it would (obviously) flatten across both dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74c37cbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda')\n",
    "\n",
    "#defining CNN class, see description below\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            nn.Conv2d(1, 5, kernel_size=(5,5)),\n",
    "            nn.MaxPool2d(kernel_size=(3,3)),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(5, 10, kernel_size=(5,5)),\n",
    "            nn.MaxPool2d(kernel_size=(3,3)),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "            \n",
    "        self.fc_layers = nn.Sequential(\n",
    "            nn.Linear(810, 80),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(80,2),\n",
    "            nn.Softmax(dim=1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_layers(x)\n",
    "        x = x.view(-1, 810) #flattens as described below\n",
    "#         x = x.view(x.size(0), -1)\n",
    "        x = self.fc_layers(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "291e8494",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 5\n",
    "\n",
    "def train(model, device, train_loader, optimizer, epoch):\n",
    "    model.train()\n",
    "    #loop run over data output by loader to train model\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data) #applies model to data\n",
    "        target = target.squeeze(1) #removes dimension from target\n",
    "        loss = nn.CrossEntropyLoss()(output, target) #calculates cross entropy loss\n",
    "        loss.backward()\n",
    "        optimizer.step() #new step in optimisation of loss\n",
    "        #printing output of each batch for loss observation while model is training\n",
    "        if batch_idx % batch_size == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100 * batch_idx / len(train_loader), loss.item()))\n",
    "\n",
    "def test(model, device, test_loader):\n",
    "    model.eval()\n",
    "    #initialisting parameters and empty lists\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    probs_pred = []\n",
    "    labels_actual = []\n",
    "    labels_pred = []\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            target=target.squeeze(1)\n",
    "            test_loss += nn.CrossEntropyLoss()(output, target).item()\n",
    "        \n",
    "            pred = output.max(1, keepdim=True)[1] #finds index of maximum in output vector as described below\n",
    "            pred_onehot = F.one_hot(pred).squeeze(1) #converts into one-hot predicted label \n",
    "            pred_bool = torch.eq(pred_onehot,target) #finds values where predicted equals target\n",
    "            correct += int((pred_bool.sum().item())/2) #sums number of correct values\n",
    "            #adds predicted labels, targets and probabilities to lists for later use\n",
    "            labels_actual.append(target.numpy())\n",
    "            probs_pred.append(output.numpy())\n",
    "            labels_pred.append(pred_onehot.numpy())\n",
    " \n",
    "    #printing accuracy and loss after each epoch for analysis\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "    100 * correct / len(test_loader.dataset)))\n",
    "    \n",
    "    return(labels_actual,probs_pred,labels_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2843f74a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch_idx, (data, target) in enumerate(train_loader):\n",
    "    print(np.shape(target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e868e8c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Net().to(device)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1, momentum=0.5) #stochastic gradient descent used as optimiser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81a6c52b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#outputting variables from initial model\n",
    "initial_model = model\n",
    "print(model)\n",
    "numel_list = [p.numel() for p in model.parameters()]\n",
    "sum(numel_list), numel_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4729148d",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 5 #setting number of epochs for CNN\n",
    "\n",
    "#running loop training and testing over previously specified number of epochs\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    train(model, device, train_loader, optimizer, epoch)\n",
    "    act,pred,pred_labs = test(model, device, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6896ebfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "febf61e7",
   "metadata": {},
   "source": [
    "# Trying another model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74b6ded8",
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = torch.randn(50, 1, 97, 97)\n",
    "m1 = nn.Conv2d(1, 5, kernel_size=(6,6))\n",
    "o1 = m1(inp)\n",
    "print(np.shape(o1))\n",
    "m2 = nn.MaxPool2d(kernel_size=(2,2))\n",
    "o2 = m2(o1)\n",
    "print(np.shape(o2))\n",
    "m3 = nn.Conv2d(5, 10, kernel_size=(5,5))\n",
    "o3 = m3(o2)\n",
    "print(np.shape(o3))\n",
    "m4 = nn.MaxPool2d(kernel_size=(2,2))\n",
    "o4 = m4(o3)\n",
    "print(np.shape(o4))\n",
    "m5 = nn.Conv2d(10, 50, kernel_size=(6,6))\n",
    "o5 = m5(o4)\n",
    "print(np.shape(o5))\n",
    "m6 = nn.MaxPool2d(kernel_size=(2,2))\n",
    "o6 = m6(o5)\n",
    "print(np.shape(o6))\n",
    "\n",
    "\n",
    "o7 = o6.view(-1, 3200)\n",
    "print(np.shape(o7))\n",
    "m8 = nn.Linear(3200,1000)\n",
    "o8 = m8(o7)\n",
    "print(np.shape(o8))\n",
    "m9 = nn.Linear(1000,300)\n",
    "o9 = m9(o8)\n",
    "print(np.shape(o9))\n",
    "m10 = nn.Linear(300,80)\n",
    "o10 = m10(o9)\n",
    "print(np.shape(o10))\n",
    "m11 = nn.Linear(80,2)\n",
    "o11 = m11(o10)\n",
    "print(np.shape(o11))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbb97152",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda')\n",
    "\n",
    "#defining CNN class, see description below\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            nn.Conv2d(1, 5, kernel_size=(6,6)),\n",
    "            nn.MaxPool2d(kernel_size=(2,2)),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(5, 10, kernel_size=(5,5)),\n",
    "            nn.MaxPool2d(kernel_size=(2,2)),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(10, 50, kernel_size=(6,6)),\n",
    "            nn.MaxPool2d(kernel_size=(2,2)),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "            \n",
    "        self.fc_layers = nn.Sequential(\n",
    "            nn.Linear(3200,1000),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1000,300),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(300,80),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(80,2),\n",
    "            nn.Softmax(dim=1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_layers(x)\n",
    "        x = x.view(-1, 3200) #flattens as described below\n",
    "#         x = x.view(x.size(0), -1)\n",
    "        x = self.fc_layers(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1944d438",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Net().to(device)\n",
    "#model.load_state_dict(torch.load('./data/mnist_cnns.pth'))\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1, momentum=0.5) #stochastic gradient descent used as optimiser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2601bc73",
   "metadata": {},
   "outputs": [],
   "source": [
    "#outputting variables from initial model\n",
    "initial_model = model\n",
    "print(model)\n",
    "numel_list = [p.numel() for p in model.parameters()]\n",
    "sum(numel_list), numel_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b52aee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 8 #setting number of epochs for CNN\n",
    "\n",
    "#running loop training and testing over previously specified number of epochs\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    train(model, device, train_loader, optimizer, epoch)\n",
    "    act,pred,pred_labs = test(model, device, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cb2b867",
   "metadata": {},
   "source": [
    "That's probably the best accuracy I can get with this much data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d0715c7",
   "metadata": {},
   "source": [
    "## More Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c597de24",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_small, X_test_small, y_train_small, y_test_small = rd.choices(X_train, k=24000), rd.choices(X_test, k=6000),\\\n",
    "rd.choices(y_train, k=24000), rd.choices(y_test, k=6000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd299eed",
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain_tensor = torch.from_numpy(np.array(X_train_small)) #converts numpy array to torch tensor\n",
    "xtrain_sqz = xtrain_tensor.unsqueeze(1) #adds new dimension along axis 1\n",
    "print(xtrain_sqz.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcc8ef78",
   "metadata": {},
   "outputs": [],
   "source": [
    "ytrain_tensor = torch.from_numpy(np.array(y_train_small))\n",
    "ytrain_sqz = ytrain_tensor.unsqueeze(1)\n",
    "print(ytrain_sqz.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e47de66",
   "metadata": {},
   "outputs": [],
   "source": [
    "xtest_tensor = torch.from_numpy(np.array(X_test_small)) #converts numpy array to torch tensor\n",
    "xtest_sqz = xtest_tensor.unsqueeze(1) #adds new dimension along axis 1\n",
    "print(xtest_sqz.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a9cb032",
   "metadata": {},
   "outputs": [],
   "source": [
    "ytest_tensor = torch.from_numpy(np.array(y_test_small))\n",
    "ytest_sqz = ytest_tensor.unsqueeze(1)\n",
    "\n",
    "print(ytest_sqz.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da8c32ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Converting data to floats and applying dataset class\n",
    "train_data = CustomDataset(xtrain_sqz.float(), ytrain_tensor.float()) #changed from sqz to float to long\n",
    "test_data = CustomDataset(xtest_sqz.float(), ytest_tensor.float())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fceb4df9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining dataloader class\n",
    "train_loader = DataLoader(dataset=train_data, batch_size=50, shuffle=True)\n",
    "train_loader_iter = iter(train_loader)\n",
    "# print(next(train_loader_iter))\n",
    "\n",
    "#testing to see shape of output\n",
    "test=next(train_loader_iter)\n",
    "test[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a4bdb18",
   "metadata": {},
   "outputs": [],
   "source": [
    "#same as above but for test data\n",
    "test_loader = DataLoader(dataset=test_data, batch_size=50, shuffle=True)\n",
    "test_loader_iter = iter(test_loader)\n",
    "\n",
    "test=next(test_loader_iter)\n",
    "test[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15cfd524",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda')\n",
    "\n",
    "#defining CNN class, see description below\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            nn.Conv2d(1, 5, kernel_size=(6,6)),\n",
    "            nn.MaxPool2d(kernel_size=(2,2)),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(5, 10, kernel_size=(5,5)),\n",
    "            nn.MaxPool2d(kernel_size=(2,2)),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(10, 50, kernel_size=(6,6)),\n",
    "            nn.MaxPool2d(kernel_size=(2,2)),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "            \n",
    "        self.fc_layers = nn.Sequential(\n",
    "            nn.Linear(3200,1000),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1000,300),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(300,80),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(80,2),\n",
    "            nn.Softmax(dim=1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_layers(x)\n",
    "        x = x.view(-1, 3200) #flattens as described below\n",
    "#         x = x.view(x.size(0), -1)\n",
    "        x = self.fc_layers(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b3a3216",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, device, train_loader, optimizer, epoch):\n",
    "    model.train()\n",
    "    #loop run over data output by loader to train model\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data) #applies model to data\n",
    "        target = target.squeeze(1) #removes dimension from target\n",
    "        loss = nn.CrossEntropyLoss()(output, target) #calculates cross entropy loss\n",
    "        loss.backward()\n",
    "        optimizer.step() #new step in optimisation of loss\n",
    "        #printing output of each batch for loss observation while model is training\n",
    "        if batch_idx % batch_size == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100 * batch_idx / len(train_loader), loss.item()))\n",
    "\n",
    "def test(model, device, test_loader):\n",
    "    model.eval()\n",
    "    #initialisting parameters and empty lists\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    probs_pred = []\n",
    "    labels_actual = []\n",
    "    labels_pred = []\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            target=target.squeeze(1)\n",
    "            test_loss += nn.CrossEntropyLoss()(output, target).item()\n",
    "        \n",
    "            pred = output.max(1, keepdim=True)[1] #finds index of maximum in output vector as described below\n",
    "            pred_onehot = F.one_hot(pred).squeeze(1) #converts into one-hot predicted label \n",
    "            pred_bool = torch.eq(pred_onehot,target) #finds values where predicted equals target\n",
    "            correct += int((pred_bool.sum().item())/2) #sums number of correct values\n",
    "            #adds predicted labels, targets and probabilities to lists for later use\n",
    "            labels_actual.append(target.numpy())\n",
    "            probs_pred.append(output.numpy())\n",
    "            labels_pred.append(pred_onehot.numpy())\n",
    " \n",
    "    #printing accuracy and loss after each epoch for analysis\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "    100 * correct / len(test_loader.dataset)))\n",
    "    \n",
    "    return(labels_actual,probs_pred,labels_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5766cf33",
   "metadata": {},
   "outputs": [],
   "source": [
    "#outputting variables from initial model\n",
    "initial_model = model\n",
    "print(model)\n",
    "numel_list = [p.numel() for p in model.parameters()]\n",
    "sum(numel_list), numel_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f5847ca",
   "metadata": {},
   "source": [
    "I've read that 3e-4 is a good learning rate for Adam. Obviously it is very much dependent on the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b153450",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Net().to(device)\n",
    "#model.load_state_dict(torch.load('./data/mnist_cnns.pth'))\n",
    "optimizer = optim.Adam(model.parameters(), lr=3e-4) #stochastic gradient descent used as optimiser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8ffb3f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 10 #setting number of epochs for CNN\n",
    "batch_size = 100\n",
    "\n",
    "#running loop training and testing over previously specified number of epochs\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    train(model, device, train_loader, optimizer, epoch)\n",
    "    act,pred,pred_labs = test(model, device, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c489e089",
   "metadata": {},
   "source": [
    "I need GPU acceleration."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31e5ed6e",
   "metadata": {},
   "source": [
    "- Use Sigmoid instead of Softmax in last layer\n",
    "- Use regularisation like L1, L2 and dropout\n",
    "- Use more convolutional layers, a larger network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e6b99f8",
   "metadata": {},
   "source": [
    "*Visualising difficulty in classification*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0723c5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in np.arange(50,61,5):\n",
    "    globals()['F_'+str(i)+'keV']=np.load('F_'+str(i)+'keV.npy') #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28f60fc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15,10))\n",
    "ax1.matshow(F_50keV[25])\n",
    "ax2.matshow(C_50keV[25]);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f84df16f",
   "metadata": {},
   "source": [
    "They're both similar and hard to classify. Theoretically it's possible but a very complex network will be necesarry with lots of data. My computer can't handle that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5e10d3c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
